{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34503cc5",
   "metadata": {},
   "source": [
    "### What is PySpark?\n",
    "PySpark is an Apache Spark interface in Python. It is used for collaborating with Spark using APIs written in Python.\n",
    "\n",
    " It also supports Spark’s features like Spark DataFrame, Spark SQL, Spark Streaming, Spark MLlib and Spark Core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b70af",
   "metadata": {},
   "source": [
    "### What is Findspark used for?\n",
    "Findspark can add a startup file to the current IPython profile so that the environment vaiables will be properly set and pyspark will be imported upon IPython startup. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94829c6",
   "metadata": {},
   "source": [
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c272727",
   "metadata": {},
   "source": [
    "### How to Initializing Spark?\n",
    "\n",
    "The first thing a Spark program is to create a SparkContext object, which tells Spark how to access a cluster. \n",
    "\n",
    "When we run any spark application, the driver program starts, which has the main() function and the SparkContext gets initiated here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1a299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500b9f2",
   "metadata": {},
   "source": [
    "### Sparksession  is the Spark Entry Point\n",
    "<a id='sparksession'></a>\n",
    "\n",
    "It contains the former\n",
    "-\tSparkcontext\n",
    "-\tSparkConf\n",
    "-\tSqlcontext\n",
    "-\tHivecontext\n",
    "-\tStreaming Context\n",
    "\n",
    "Used for:\n",
    "\n",
    "- metadata acess\n",
    "- read data \n",
    "- conf session\n",
    "- manage cluster resources (YARN,MESOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25745127",
   "metadata": {},
   "source": [
    "### Determine Spark version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6473dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version = 3.0.2\n"
     ]
    }
   ],
   "source": [
    "# spark\n",
    "print(f\"Spark version = {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e29bf",
   "metadata": {},
   "source": [
    "**What is PySpark SparkContext?**\n",
    "\n",
    "PySpark SparkContext is an initial entry point of the spark functionality. It also represents Spark Cluster Connection and can be used for creating the Spark **RDD**s (**R**esilient **D**istributed **D**atasets) and broadcasting the variables on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da11b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02278b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93864613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version #Retrieve SparkContext version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b20a231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pythonVer #Retrieve Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a8120e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master #Master URL to connect to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff65d2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sc.sparkHome) #Path where Spark is installed an worker nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2402321e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rusla'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sc.sparkUser()) #Retrieve name of the Spark User running SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fd6585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python Spark SQL basic example'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName #Return application name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1ddc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1643013259822'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.applicationId #Retrieve application ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5037e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism #Return default level of parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d636d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultMinPartitions #Default minimum number of partitions for RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e469cd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x19d93300910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e0535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apache Spark allow have one Sparkcontect on one node\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420dcaf",
   "metadata": {},
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c121fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "            .setMaster(\"local\")\n",
    "            .setAppName(\"My app\")\n",
    "            .set(\"spark.executor.memory\",\"1g\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47f79d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93b2e1",
   "metadata": {},
   "source": [
    "**Using The Shell**\n",
    "\n",
    "In the PySpark shell, a special interpreter aware SparkContext is already created in the variable called sc.\n",
    "\n",
    "```\n",
    "$ ./bin/spark shell --master local[2]\n",
    "$ ./bin/pyspark --master local[4] --py files code.py\n",
    "```\n",
    "\n",
    "Set which master the context connects to with the --master argument, and add Python **.zip, .egg** or **.py** files to the  runtime path by passing a comma separated list to --py-files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b438824",
   "metadata": {},
   "source": [
    "**What is spark-submit?**\n",
    "Spark-submit is a utility to run a pyspark application job by specifying options and configurations.\n",
    "\n",
    "    \n",
    "where configurations are:\n",
    "```python   \n",
    "spark-submit \\\n",
    "--master <master-url> \\\n",
    "--deploy-mode <deploy-mode> \\\n",
    "--conf <key<=<value> \\\n",
    "--driver-memory <value>g \\\n",
    "--executor-memory <value>g \\\n",
    "--executor-cores <number of cores> \\\n",
    "--jars <comma separated dependencies> \\\n",
    "--packages <package name> \\\n",
    "--py-files \\\n",
    "<application> <application args>\n",
    "```   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b713b",
   "metadata": {},
   "source": [
    "**How you can run pyspark program?**\n",
    "\n",
    "You can use jupyter notebook as in this tutorial we are doing, or just create a python file under bin folder. Write above codes in the .py file.  Execute the file using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83810df8",
   "metadata": {},
   "source": [
    "```bash \n",
    "spark-submit example.py\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09380da",
   "metadata": {},
   "source": [
    "### What are the Data structures in Pyspark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14effb4b",
   "metadata": {},
   "source": [
    "\n",
    "- RDD :  Data is organized into named columns, like a table in a relational database.  (untyped)\n",
    "  - Transformations\n",
    "    - collect : RDD-->  LIST\n",
    "    - MAP     : RDD --> PipelinedRDD\n",
    "    - filter  : RDD --> LIST\n",
    "  - Actions\n",
    "  \n",
    "- DF : DataFrame is an immutable distributed collection of data (untyped)\n",
    "\n",
    "- DS : Dataset (typed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11cf71",
   "metadata": {},
   "source": [
    "DataFrame and Dataset APIs are built on top of the Spark SQL engine, it uses Catalyst to generate an optimized logical and physical query plan. \n",
    "\n",
    "Across R, Java, Scala, or Python DataFrame/Dataset APIs, all relation type queries undergo the same code optimizer, providing the space and speed efficiency.\n",
    "\n",
    "Whereas the Dataset[T] typed API is optimized for data engineering tasks, the untyped Dataset[Row] (an alias of DataFrame) is even faster and suitable for interactive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc999d",
   "metadata": {},
   "source": [
    "**How to read a .csv file and infer schema?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c46991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession        \n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"App\")\n",
    "  .getOrCreate())\n",
    "# Path to data set\n",
    "csv_file = \"data/departuredelays.csv\"\n",
    "df = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2442752",
   "metadata": {},
   "source": [
    "Now that we have a temporary view, we can issue SQL queries using Spark SQL. These queries are no different from those you might issue against a SQL table in, say, a MySQL or PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0050f4",
   "metadata": {},
   "source": [
    "Next, we’ll find all flights between San Francisco (SFO) and Chicago (ORD) with at least a two-hour delay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ec562f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984aab3c",
   "metadata": {},
   "source": [
    "As with the DataFrame and Dataset APIs, with the spark.sql interface you can conduct common data analysis operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd300fb3",
   "metadata": {},
   "source": [
    "We’ll find all flights whose distance is greater than 1,000 miles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "552cc02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    "  .where(col(\"distance\") > 1000)\n",
    "  .orderBy(desc(\"distance\"))).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f8017",
   "metadata": {},
   "source": [
    "**What are RDDs in PySpark?**\n",
    "\n",
    "RDDs expand to Resilient Distributed Datasets. These are the elements that are used for running and operating on multiple nodes to perform parallel processing on a cluster. Since RDDs are suited for parallel processing, they are immutable elements. This means that once we create RDD, we cannot modify it. RDDs are also fault-tolerant which means that whenever failure happens, they can be recovered automatically. Multiple operations can be performed on RDDs to perform a certain task.\n",
    "\n",
    "- RDD is a distributed collection of data elements without any schema\n",
    "- No in-built optimization engine for RDDs\n",
    "- We need to define the schema manually.\n",
    "- RDD is slower than both Dataframes and Datasets to perform simple operations like grouping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aee741",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "**Parallelized Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4730ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d24d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7fc92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "535cc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = sc.parallelize([(\"a\",[\"x\",\"y\",\"z\"]),\n",
    "                            (\"b\",[\"p\",\"r\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c56a3",
   "metadata": {},
   "source": [
    "**External Data**\n",
    "\n",
    "Read either one text file from HDFS.a local file system or or any Hadoop-supported file system URI with textFile(). or read in a directory of text files with wholeTextFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8b196f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"data/people.txt\")\n",
    "textFile2 = sc.wholeTextFiles( \"data/Orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d2805",
   "metadata": {},
   "source": [
    "### Retrieving RDD Information\n",
    "\n",
    "**Basic Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ec5e5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions() #List the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd19f357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count() #Count ROD instances 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "970c5d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey() #Count ROD instances by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f718c064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue() #Count ROD instances by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "075e74b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 2}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collectAsMap() #Return (key,value) pairs as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f063f465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.sum() #Sum of ROD elements 4950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fca44c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([]).isEmpty() #Check whether ROD is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f9e1f",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22dd9d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.max() #Maximum value of ROD elements 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b84c4cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.min() #Minimum value of ROD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db23736f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.mean() #Mean value of ROD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ee0b035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.86607004772212"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.stdev() #Standard deviation of ROD elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d63ded0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833.25"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.variance() #Compute variance of ROD elements 833.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7043d99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 33, 66, 99], [33, 33, 34])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.histogram(3) #Compute histogram by bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4e4ee30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.stats() #Summary statistics (count, mean, stdev, max & min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311e500",
   "metadata": {},
   "source": [
    "**Applying Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63343544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply a function to each ROD element\n",
    "rdd.map(lambda x: x+(x[1],x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05373598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply a function to each ROD element and flatten the result\n",
    "rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "701d0e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 7, 7, 'a', 'a', 2, 2, 'a', 'b', 2, 2, 'b']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9991b8c9",
   "metadata": {},
   "source": [
    "**Selecting Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47b8f0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() #Return a list with all ROD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b9bda02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rdd.take(2) #Take first 2 ROD elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9758c27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first() #Toke first ROD element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c56fd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 7)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(2) #Take top 2 ROD elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ceddea",
   "metadata": {},
   "source": [
    "**Creation of  RDD using textFile API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e89f0e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a2b79ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We can create a RDD from file\n",
    "rdd6 = spark.sparkContext.textFile('data/people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "159f19fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Andy, 30', 'Justin, 19']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now that it is created the rdd we can take the first 5 elements with take()\n",
    "rdd6.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e02e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael, 29\n",
      "Andy, 30\n",
      "Justin, 19\n"
     ]
    }
   ],
   "source": [
    "#We can print the elements that we have taken\n",
    "for i in rdd6.take(5): \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4ea85",
   "metadata": {},
   "source": [
    "**Get the Number of Partitions in the RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e32231a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267b05d",
   "metadata": {},
   "source": [
    "**Create a RDD from a Python List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88bf2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "lst = [1,2,3,4,5,6,7]\n",
    "rdd7 = spark.sparkContext.parallelize(lst)\n",
    "for i in rdd7.take(5) : \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325923e9",
   "metadata": {},
   "source": [
    "**Creation of rdd manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "821e9152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Amber', 22), ('Alfred', 23), ('Skye', 4), ('Albert', 12), ('Amber', 9)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8 = sc.parallelize(\n",
    "    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), \n",
    "     ('Amber', 9)])\n",
    "print(type(rdd8))\n",
    "rdd8.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d631c1",
   "metadata": {},
   "source": [
    "**Create a RDD from local file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dff5ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael, 29\n",
      "Andy, 30\n",
      "Justin, 19\n"
     ]
    }
   ],
   "source": [
    "lst = open('data/people.txt').read().splitlines()\n",
    "lst[0:10]\n",
    "rdd9 = spark.sparkContext.parallelize(lst)\n",
    "for i in rdd9.take(5) : print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b858ad7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading from file taking 4 partitions\n",
    "data_from_file = sc.textFile('data/people.txt',4)\n",
    "type(data_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e0e75",
   "metadata": {},
   "source": [
    "**how many partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad1b7b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file.getNumPartitions() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58058b03",
   "metadata": {},
   "source": [
    "**Create RDD using textFile API and a defined number of partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b1f189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
      "2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n",
      "3,2013-07-25 00:00:00.0,12111,COMPLETE\n",
      "4,2013-07-25 00:00:00.0,8827,CLOSED\n",
      "5,2013-07-25 00:00:00.0,11318,COMPLETE\n"
     ]
    }
   ],
   "source": [
    "rdd10 = spark.sparkContext.textFile('data/Orders',10)\n",
    "for i in rdd10.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4926bb",
   "metadata": {},
   "source": [
    "**Create RDD from range function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "270f7cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "lst1 = range(10)\n",
    "rdd11 = spark.sparkContext.parallelize(lst1)\n",
    "for i in rdd11.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3565b",
   "metadata": {},
   "source": [
    "**What are Dataframes?**\n",
    "\n",
    "It was introduced first in Spark version 1.3 to overcome the limitations of the Spark RDD. Spark Dataframes are the distributed collection of the data points, but here, the data is organized into the named columns\n",
    "\n",
    "- It is also the distributed collection organized into the named columns\n",
    "- It uses a catalyst optimizer for optimization.\n",
    "- It will automatically find out the schema of the dataset.\n",
    "- It performs aggregation faster than both RDDs and Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6953c9",
   "metadata": {},
   "source": [
    "**Create RDD from a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2097ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|robert| 35|\n",
      "|  Mike| 45|\n",
      "+------+---+\n",
      "\n",
      "Row(name='robert', age=35)\n",
      "Row(name='Mike', age=45)\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(data=(('robert',35),('Mike',45)),schema=('name','age'))\n",
    "df.printSchema()\n",
    "df.show()\n",
    "rdd11= df.rdd\n",
    "type(rdd11)\n",
    "for i in rdd11.take(2) : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86c0f4",
   "metadata": {},
   "source": [
    "**What are Datasets?**\n",
    "\n",
    "Spark Datasets is an extension of Dataframes API with the benefits of both RDDs and the Datasets. It is fast as well as provides a type-safe interface. \n",
    "\n",
    "- It is an extension of Dataframes with more features like type-safety and object-oriented interface.\n",
    "- It uses a catalyst optimizer for optimization.\n",
    "- It will automatically find out the schema of the dataset.\n",
    "-  Dataset is faster than RDDs but a bit slower than Dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af99b0",
   "metadata": {},
   "source": [
    "**What type of operation has Pyspark?**\n",
    "\n",
    "The operations can be of 2 types, actions and transformation.\n",
    "\n",
    "**What is Transformation in Pyspark?**\n",
    "\n",
    "Transformation: These operations when applied on RDDs result in the creation of a new RDD. Some of the examples of transformation operations are filter, groupBy, map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728d793",
   "metadata": {},
   "source": [
    "![](data/transformation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854af501",
   "metadata": {},
   "source": [
    "**Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcb329ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: \"a\" in x).collect() #Filter the ROD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29ea1848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 7, 2, 'b']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.distinct().collect() #Return distinct ROD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4792f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.keys().collect() #Return (key,value) RDD's keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a48f5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x): print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be1017b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(g) #Apply a function to all ROD elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c73c2",
   "metadata": {},
   "source": [
    "### Reshaping Data\n",
    "\n",
    "**Reducing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80b82978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 9), ('b', 2)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x,y : x+y).collect() #Merge the rdd values for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5491be7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7, 'a', 2, 'b', 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda a, b: a+b) #Merge the rdd values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6ec6d",
   "metadata": {},
   "source": [
    "**Grouping by**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0704190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupped=rdd3.groupBy(lambda x: x % 2) \\\n",
    " .mapValues(list)\\\n",
    " .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c06d3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [7, 2]), ('b', [2])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey() \\\n",
    " .mapValues(list)\\\n",
    " .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704926a",
   "metadata": {},
   "source": [
    "**Aggregating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f55e0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqOp = (lambda x,y: (x[0]+y,x[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0f78495",
   "metadata": {},
   "outputs": [],
   "source": [
    "combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8171ec9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4950, 100)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aggregate RDD elements of each partition and then the results\n",
    "rdd3.aggregate((0,0),seqOp,combOp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d015c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (9, 2)), ('b', (2, 1))]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aggregate values of each RDD key\n",
    "rdd.aggregateByKey((0,0),seqOp,combOp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "095673a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tuples of RDD elements by applying a function\n",
    "tuple=rdd3.keyBy(lambda x: x+x).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03241b",
   "metadata": {},
   "source": [
    "### Mathematical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e968c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('b', 2)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.subtract(rdd2).collect() #Return each rdd value not contained in rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b615a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return each (key,value) pair of rdd2 with no matching key in rdd\n",
    "rdd2.subtractByKey(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e491f6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 7), ('a', 2)),\n",
       " (('a', 7), ('d', 1)),\n",
       " (('a', 7), ('b', 1)),\n",
       " (('a', 2), ('a', 2)),\n",
       " (('a', 2), ('d', 1)),\n",
       " (('a', 2), ('b', 1)),\n",
       " (('b', 2), ('a', 2)),\n",
       " (('b', 2), ('d', 1)),\n",
       " (('b', 2), ('b', 1))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cartesian(rdd2).collect() #Return the Cartesian product of rdd and rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12aa341",
   "metadata": {},
   "source": [
    "**Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dfc973e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 1), ('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.sortBy(lambda x: x[1]).collect() #Sort ROD by given function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "825d3e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1), ('d', 1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.sortByKey().collect() #Sort (key, value) ROD by key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c926b",
   "metadata": {},
   "source": [
    "Let us take an example to demonstrate transformation operation by considering filter() operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0810d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = sc.parallelize (\n",
    "  [\"pyspark\", \n",
    "  \"interview\", \n",
    "  \"questions\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a48aeeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interview']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = words_list.filter(lambda x: 'interview' in x)\n",
    "filtered = filtered_words.collect()\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9004ee1",
   "metadata": {},
   "source": [
    "**What is map?**\n",
    "\n",
    "Perform row level transformations where one record transforms into another record. Number of records in input is equal to output, Return a new RDD by applying a function to each element of this RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94269ac",
   "metadata": {},
   "source": [
    "**View RDD contents in Python Spark**\n",
    " take / collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6fa66",
   "metadata": {},
   "source": [
    "When you call collect() or take(), you get back a list of elements in the rdd. You can then print these values as you would any normal python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f0309b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f030e847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=data_from_file.collect()\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca244896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Michael, 29', 'Andy, 30', 'Justin, 19']\n"
     ]
    }
   ],
   "source": [
    "a=data_from_file.map(lambda row: row)\n",
    "b=a.collect()\n",
    "print(type(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066c3e7",
   "metadata": {},
   "source": [
    "**Project all the Order_ids by using map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1169176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "ord = sc.textFile('data/Orders')\n",
    "ordMap = ord.map(lambda x : x.split(',')[0])\n",
    "for i in ordMap.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a27b9",
   "metadata": {},
   "source": [
    "**Project all the Orders and their status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "536a4c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'CLOSED'),\n",
       " ('2', 'PENDING_PAYMENT'),\n",
       " ('3', 'COMPLETE'),\n",
       " ('4', 'CLOSED'),\n",
       " ('5', 'COMPLETE')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord.map(lambda x : (x.split(',')[0],x.split(',')[3])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd901c",
   "metadata": {},
   "source": [
    "**Combine Order id and status with '#'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c18cb7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1#CLOSED', '2#PENDING_PAYMENT', '3#COMPLETE', '4#CLOSED', '5#COMPLETE']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord.map(lambda x : x.split(',')[0] + '#' + x.split(',')[3]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5f18c",
   "metadata": {},
   "source": [
    "**Convert the Order date into YYYY/MM/DD Format.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a686aa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2013/07/25'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord.map(lambda x : x.split(',')[1].split(' ')[0].replace('-','/')).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a32d9",
   "metadata": {},
   "source": [
    "**Print Five elements of the RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ec9b35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = ord.map(lambda x: (x, 1))\n",
    "mapreduce = map.reduceByKey(lambda x,y: x+y)\n",
    "result = mapreduce.collect()\n",
    "top5 = result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7fbaa5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1,2013-07-25 00:00:00.0,11599,CLOSED', 1),\n",
       " ('2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT', 1),\n",
       " ('3,2013-07-25 00:00:00.0,12111,COMPLETE', 1),\n",
       " ('4,2013-07-25 00:00:00.0,8827,CLOSED', 1),\n",
       " ('5,2013-07-25 00:00:00.0,11318,COMPLETE', 1)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b55c1",
   "metadata": {},
   "source": [
    "**Applied user defined function to convert status into lowercase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96de1f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerCase(str):\n",
    "    return str.lower()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "06642669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'closed'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord.map(lambda x : lowerCase(x.split(',')[3])).first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b434bab",
   "metadata": {},
   "source": [
    "**What is flatMap?**\n",
    "\n",
    "Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item). Number of records in input is less than or equal to output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd676093",
   "metadata": {},
   "source": [
    "**Word count in orders file using flatMap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9bfe5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord = sc.textFile('data/Orders')\n",
    "wordCount = ord.flatMap(lambda x : x.split(',')).map(lambda w : (w,1)).reduceByKey(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7378bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  flatMap  [used to extract word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b859f05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['M', 'i', 'c', 'h', 'a', 'e', 'l', ',', ' ', '2', '9', 'A', 'n', 'd', 'y', ',', ' ', '3', '0', 'J', 'u', 's', 't', 'i', 'n', ',', ' ', '1', '9']\n"
     ]
    }
   ],
   "source": [
    "a=data_from_file.flatMap(lambda row: row)\n",
    "b=a.collect()\n",
    "print(type(b))\n",
    "print(b)\n",
    "#a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf4a6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLATMAP\n",
    "wordCount =data_from_file.flatMap(lambda line: line.split()).map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "55531645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordCount: [('Andy,', 1), ('Michael,', 1), ('30', 1), ('29', 1), ('Justin,', 1), ('19', 1)] \n"
     ]
    }
   ],
   "source": [
    "print('wordCount: %s ' % wordCount.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac09cd0",
   "metadata": {},
   "source": [
    "**What is filter?**\n",
    "\n",
    "Return a new dataset formed by selecting those elements of the source on which func returns true.\n",
    "\n",
    "Print all the orders which are closed or Complete and ordered in the year 2013 by using filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "843f222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord = sc.textFile('Orders')\n",
    "filteredOrd = ord.filter(lambda x : (x.split(',')(3) in (\"CLOSED\",\"COMPLETE\")) and (x.split(',')(1).split('-')(0) == '2014'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "76b25743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Andy, 30']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering\n",
    "data_from_file.filter(lambda row: row[0] == 'A').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6bad3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aa6ff97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'M', 'A']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file.map(lambda row: row[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ac72f",
   "metadata": {},
   "source": [
    " **What is mapValues?**\n",
    "\n",
    "The difference with map is map operates on the entire record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4a18dcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 3), ('a', 5)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(((\"a\", (1,2,3)), (\"b\", (3,4,5)),(\"a\", (1,2,3,4,5))))\n",
    "def f(x): return len(x)\n",
    "rdd.mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a6bed",
   "metadata": {},
   "source": [
    "**What is join in pyspark?**\n",
    "\n",
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5bcf4",
   "metadata": {},
   "source": [
    " **Find the subtotal for each ORDER_CUSTOMER_ID by using join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b7dc5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord = sc.textFile('data/Orders')\n",
    "ordItems = sc.textFile('data/Order_items')\n",
    "ordMap=ord.map(lambda x : (x.split(',')(0),x.split(',')(2)))\n",
    "ordItemsMap=ordItems.map(lambda x : (x.split(',')(1),x.split(',')(4)))\n",
    "findSubtotalForCust = ordMap.join(ordItemsMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018ee3a",
   "metadata": {},
   "source": [
    "**join / leftouterjoin / intersection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "22815bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (4, '6')), ('c', (10, None)), ('a', (1, 4)), ('a', (1, 1))]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a', 1), ('b', 4), ('c',10)])\n",
    "rdd2 = sc.parallelize([('a', 4), ('a', 1), ('b', '6'), ('d', 15)])\n",
    "\n",
    "rdd3 = rdd1.leftOuterJoin(rdd2)\n",
    "rdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9674ecf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (4, '6')), ('a', (1, 4)), ('a', (1, 1))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = rdd1.join(rdd2)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a88727",
   "metadata": {},
   "source": [
    "**reduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fc64cd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cds'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file.map(lambda row: row[2]).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c702e04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76544f",
   "metadata": {},
   "source": [
    "**What is cogroup in pyspark?**\n",
    "\n",
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable, Iterable)) tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75e5be",
   "metadata": {},
   "source": [
    "**Give me an example of cogroup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "08d2db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize(((\"a\", 1), (\"b\", 4)))\n",
    "y = sc.parallelize(((\"a\", 2)))\n",
    "xy = x.cogroup(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b20f9",
   "metadata": {},
   "source": [
    "**What is cartesian in Pyspark?**\n",
    "\n",
    "Perform a cross join. Give me an example of cartesion in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c3684733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', (1, 2, 3)), ('a', (1, 2, 3))),\n",
       " (('a', (1, 2, 3)), ('a', (1, 2, 3, 4, 5))),\n",
       " (('a', (1, 2, 3)), ('b', (3, 4, 5))),\n",
       " (('a', (1, 2, 3, 4, 5)), ('a', (1, 2, 3))),\n",
       " (('a', (1, 2, 3, 4, 5)), ('a', (1, 2, 3, 4, 5))),\n",
       " (('a', (1, 2, 3, 4, 5)), ('b', (3, 4, 5))),\n",
       " (('b', (3, 4, 5)), ('a', (1, 2, 3))),\n",
       " (('b', (3, 4, 5)), ('a', (1, 2, 3, 4, 5))),\n",
       " (('b', (3, 4, 5)), ('b', (3, 4, 5)))]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd13 = sc.parallelize((1,3,2))\n",
    "sorted(rdd.cartesian(rdd).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc1df8",
   "metadata": {},
   "source": [
    "**Give me some Aggregation Operations in Spark**\n",
    "\n",
    "There are several APIs to perform aggregation Operations. •\n",
    "\n",
    "- Total aggregations – **reduce, count (Actions)** • \n",
    "\n",
    "- By Key aggregations – **reduceByKey, aggregrateByKey, groupByKey, countByKey (Transformations)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9165855",
   "metadata": {},
   "source": [
    "**What is the aggregation reduce?**\n",
    "\n",
    " Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally\n",
    "\n",
    "**What is the aggregration count?**\n",
    "\n",
    "Return the number of elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "53c5a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bc35a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data\n",
    "data = [(10, {3: 3.616726727464709, 4: 2.9996439803387602, 5: 1.6767412921625855}),\n",
    "        (1, {3: 2.016527311459324, 4: -1.5271512313750577, 5: 1.9665475696370045}),\n",
    "        (2, {3: 6.230272144805092, 4: 4.033642544526678, 5: 3.1517805604906313}),\n",
    "        (3, {3: -0.3924680103722977, 4: 2.9757316477407443, 5: -1.5689126834176417})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e9c494d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your rdd\n",
    "rdd14 = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4b94dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to spark data frame\n",
    "df = rdd14.toDF([\"CId\", \"Values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d4c2d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CId: long (nullable = true)\n",
      " |-- Values: map (nullable = true)\n",
      " |    |-- key: long\n",
      " |    |-- value: double (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7290e0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "|CId|IID|              Score|\n",
      "+---+---+-------------------+\n",
      "| 10|  3|  3.616726727464709|\n",
      "| 10|  4| 2.9996439803387602|\n",
      "| 10|  5| 1.6767412921625855|\n",
      "|  1|  3|  2.016527311459324|\n",
      "|  1|  4|-1.5271512313750577|\n",
      "|  1|  5| 1.9665475696370045|\n",
      "|  2|  3|  6.230272144805092|\n",
      "|  2|  4|  4.033642544526678|\n",
      "|  2|  5| 3.1517805604906313|\n",
      "|  3|  3|-0.3924680103722977|\n",
      "|  3|  4| 2.9757316477407443|\n",
      "|  3|  5|-1.5689126834176417|\n",
      "+---+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use explode\n",
    "df.select(\"CId\", explode(\"Values\").alias(\"IID\", \"Score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1233d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. flatten your data 2. put it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "99211635",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd15 = rdd14.flatMapValues(lambda x : [ (k, x[k]) for k in x.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4be80bf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "|CId|IID|              Score|\n",
      "+---+---+-------------------+\n",
      "| 10|  3|  3.616726727464709|\n",
      "| 10|  4| 2.9996439803387602|\n",
      "| 10|  5| 1.6767412921625855|\n",
      "|  1|  3|  2.016527311459324|\n",
      "|  1|  4|-1.5271512313750577|\n",
      "|  1|  5| 1.9665475696370045|\n",
      "|  2|  3|  6.230272144805092|\n",
      "|  2|  4|  4.033642544526678|\n",
      "|  2|  5| 3.1517805604906313|\n",
      "|  3|  3|-0.3924680103722977|\n",
      "|  3|  4| 2.9757316477407443|\n",
      "|  3|  5|-1.5689126834176417|\n",
      "+---+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd15.map(lambda x : (x[0], x[1][0], x[1][1]))\\\n",
    "    .toDF((\"CId\", \"IID\", \"Score\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac5118",
   "metadata": {},
   "source": [
    "**What is Action in Pyspark?**\n",
    "\n",
    "Action: These operations instruct Spark to perform some computations on the RDD and return the result to the driver. It sends data from the Executer to the driver. count(), collect(), take() are some of the examples.\n",
    "Let us consider an example to demonstrate action operation by making use of the count() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "96431a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of elements in RDD ->  3\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "  [\"pyspark\", \n",
    "  \"interview\", \n",
    "  \"questions\"]\n",
    ")\n",
    "counts = words.count()\n",
    "print(\"Count of elements in RDD -> \",  counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531dc0b",
   "metadata": {},
   "source": [
    "we count the number of elements in the spark RDDs. The output of this code is Count of elements in RDD -> 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a26af",
   "metadata": {},
   "source": [
    "![](data/actions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856bcab",
   "metadata": {},
   "source": [
    "\n",
    "## Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dc98ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20a21b",
   "metadata": {},
   "source": [
    "**DataFrame from RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a54ba7",
   "metadata": {},
   "source": [
    "```python \n",
    "# Import PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local[1]\")\\\n",
    "                    .appName(\"App\")\\\n",
    "                    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3e3b16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd16 = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e0ca3",
   "metadata": {},
   "source": [
    "**Using toDF() function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "14e767f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfFromRDD1 = rdd16.toDF()\n",
    "dfFromRDD1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b571f",
   "metadata": {},
   "source": [
    "Using createDataFrame() from SparkSession is another way to create manually and it takes rdd object as an argument. and chain with toDF() to specify name to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6a5b2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromRDD2 = spark.createDataFrame(rdd16).toDF(*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d89174",
   "metadata": {},
   "source": [
    "**Create DataFrame with schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "be311439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b6bd2",
   "metadata": {},
   "source": [
    "**Infer Schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4adb49",
   "metadata": {},
   "source": [
    "```python \n",
    "from pyspark import SparkContext\n",
    "# Create the Spark context.\n",
    "sc = SparkContext()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2876cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d9cdcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "df = sqlContext.read.json(\"data/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f0e440f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the content of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1bb823f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fc99fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f113d4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f3d4bce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "850f9f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4597fca",
   "metadata": {},
   "source": [
    "**Specify Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "39162b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  k|  v|\n",
      "+---+---+\n",
      "|foo|  1|\n",
      "|bar|  2|\n",
      "+---+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame([(\"foo\", 1), (\"bar\", 2), (\"baz\", 3)], ('k', 'v'))\n",
    "df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b618f9",
   "metadata": {},
   "source": [
    "**From Spark Data Sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8d1c9",
   "metadata": {},
   "source": [
    "**Parquet files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0588f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"data/users.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddfb529",
   "metadata": {},
   "source": [
    "**JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d7c60249",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load( \"data/people.json\" , format= \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb720b",
   "metadata": {},
   "source": [
    "Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "63e4ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"age\"] >24).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91b36e",
   "metadata": {},
   "source": [
    "**Duplicate Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a1172025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce194b59",
   "metadata": {},
   "source": [
    "### Queries\n",
    "\n",
    "\n",
    "**What is PySpark SQL?**\n",
    "PySpark SQL is the most popular PySpark module that is used to process structured columnar data. Once a DataFrame is created, we can interact with data using the SQL syntax. Spark SQL is used for bringing native raw SQL queries on Spark by using select, where, group by, join, union etc. For using PySpark SQL, the first step is to create a temporary table on DataFrame by using createOrReplaceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0ab8df6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.appName('App').getOrCreate()\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8fd46942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e835e1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|firstName|\n",
      "+---------+\n",
      "|    James|\n",
      "|  Michael|\n",
      "|   Robert|\n",
      "|    Maria|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select( \"firstName\").show() #Show all entries in firstNome column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "223ce043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"firstname\",\"lastname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b53b2a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.firstname,df.lastname).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c469ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(df[\"firstname\"],df[\"lastname\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a3525879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#By using col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"firstname\"),col(\"lastname\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b3878431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Select columns by regular expression\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "58aeb724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select All columns from List\n",
    "df.select(*columns).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c38f4bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select All columns\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7825e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+\n",
      "|firstname|lastname|country|\n",
      "+---------+--------+-------+\n",
      "|    James|   Smith|    USA|\n",
      "|  Michael|    Rose|    USA|\n",
      "|   Robert|Williams|    USA|\n",
      "+---------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selects first 3 columns and top 3 rows\n",
    "df.select(df.columns[:3]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ab877069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|country|state|\n",
      "+-------+-----+\n",
      "|    USA|   CA|\n",
      "|    USA|   NY|\n",
      "|    USA|   CA|\n",
      "+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selects columns 2 to 4  and top 3 rows\n",
    "df.select(df.columns[2:4]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "821c3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d74ab3",
   "metadata": {},
   "source": [
    "Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0d5fe77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d938bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# Generate our own CSV data \n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),    \n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Apply the schema to the RDD and Create DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2faff65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### take show collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ae1ff7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "88af2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b7a0ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|distinct|\n",
      "+-----+--------+\n",
      "|    7|       5|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "        (1, 144.5, 5.9, 33, 'M'),\n",
    "        (2, 167.2, 5.4, 45, 'M'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (4, 144.5, 5.9, 33, 'M'),\n",
    "        (5, 133.2, 5.7, 54, 'F'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (5, 129.2, 5.3, 42, 'M'),\n",
    "    ], ['id', 'weight', 'height', 'age', 'gender'])\n",
    "\n",
    "df.agg(\n",
    "    fn.count('id').alias('count'),\n",
    "    fn.countDistinct('id').alias('distinct')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3ff95f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dfb7a8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Query\n",
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fc1e2475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query id and age for swimmers with age = 22 via DataFrame API in another way\n",
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()\n",
    "# Query id and age for swimmers with age = 22 in SQL\n",
    "spark.sql(\"select id, age from swimmers where age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0590a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Primary Key Artificial /Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "67a7a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+------+\n",
      "| id|weight|height|age|gender|new_id|\n",
      "+---+------+------+---+------+------+\n",
      "|  1| 144.5|   5.9| 33|     M|     0|\n",
      "|  2| 167.2|   5.4| 45|     M|     1|\n",
      "|  3| 124.1|   5.2| 23|     F|     2|\n",
      "|  4| 144.5|   5.9| 33|     M|     3|\n",
      "|  5| 133.2|   5.7| 54|     F|     4|\n",
      "|  3| 124.1|   5.2| 23|     F|     5|\n",
      "|  5| 129.2|   5.3| 42|     M|     6|\n",
      "+---+------+------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "df.withColumn('new_id', fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4ee99",
   "metadata": {},
   "source": [
    "### What is GroupBy\n",
    "\n",
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable) pairs. • If grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance. • The number of reduce tasks is configurable through an optional argument – numPartitions\n",
    "\n",
    "For each product, find its aggregated revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7fd281b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordItems = sc.textFile('data/Order_items')\n",
    "ordGrp = ordItems.map(lambda x : (int(x.split(',')[2]),float(x.split(',')[4]))).groupByKey()\n",
    "result = ordGrp.mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cd63ea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 54|    1|\n",
      "| 33|    2|\n",
      "| 42|    1|\n",
      "| 23|    2|\n",
      "| 45|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group by age, count the members in the groups\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b4d16",
   "metadata": {},
   "source": [
    "What is reduceByKey?\n",
    "\n",
    "• Like in groupByKey, the number of reduce tasks is configurable through an optional argument - numPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6e5bb29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize(((\"a\", 1), (\"b\", 1), (\"a\", 1)))\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea70e5",
   "metadata": {},
   "source": [
    "**What is aggregateByKey?**\n",
    "\n",
    "First aggregate elements in each partition and then aggregating results of all partition to get the final result and the result could be any type than the type of your RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "66f71e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordItems=sc.parallelize([\n",
    "(2,\"Joseph\",200), (2,\"Jimmy\",250), (2,\"Tina\",130), (4,\"Jimmy\",50), (4,\"Tina\",300),\n",
    "(4,\"Joseph\",150), (4,\"Ram\",200), (7,\"Tina\",200), (7,\"Joseph\",300), (7,\"Jimmy\",80)],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a3de1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Paired RDD\n",
    "ordPair = ordItems.map(lambda x : (x[0],(x[1],x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "27be5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Accumulator\n",
    "# Zero Value: Zero value in our case will be 0 as we are finding Maximum Marks\n",
    "zero_val=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "85fdc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Sequence Operation\n",
    "# Sequence operation : Finding Maximum revenue from each partition\n",
    "def seq_op(accumulator, element):\n",
    "    if(accumulator > element[1]):\n",
    "        return accumulator\n",
    "    else:\n",
    "        return element[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "568d7ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Combiner Operation\n",
    "#Combiner Operation : Finding Maximum revenue from all partitions\n",
    "def comb_op(accumulator1, accumulator2):\n",
    "    if(accumulator1 > accumulator2):\n",
    "        return accumulator1\n",
    "    else:\n",
    "        return accumulator2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ee1d33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_ordItems = ordPair.aggregateByKey(zero_val, seq_op, comb_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "998ece8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 250)\n",
      "(4, 300)\n",
      "(7, 300)\n"
     ]
    }
   ],
   "source": [
    "for i in aggr_ordItems.collect(): print(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687e379",
   "metadata": {},
   "source": [
    "**what is countByKey()?**\n",
    "\n",
    " Only available on RDDs of type (K, V). Returns a (K, Int) pairs with the count of each key. Returns a Collection Dictionary.  No shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e770f",
   "metadata": {},
   "source": [
    "**what is sortByKey?**\n",
    "\n",
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the Boolean ascending argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b0cec25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12435, '41643,2014-04-08 00:00:00.0,12435,PENDING')\n",
      "(12435, '61629,2013-12-21 00:00:00.0,12435,CANCELED')\n",
      "(12434, '1868,2013-08-03 00:00:00.0,12434,CLOSED')\n",
      "(12434, '4799,2013-08-23 00:00:00.0,12434,PENDING_PAYMENT')\n",
      "(12434, '5303,2013-08-26 00:00:00.0,12434,PENDING')\n",
      "(12434, '6160,2013-09-02 00:00:00.0,12434,COMPLETE')\n",
      "(12434, '13544,2013-10-16 00:00:00.0,12434,PENDING')\n",
      "(12434, '42915,2014-04-16 00:00:00.0,12434,COMPLETE')\n",
      "(12434, '51800,2014-06-14 00:00:00.0,12434,ON_HOLD')\n",
      "(12434, '61777,2013-12-26 00:00:00.0,12434,COMPLETE')\n"
     ]
    }
   ],
   "source": [
    "ord = sc.textFile('data/Orders')\n",
    "ordPair = ord.map(lambda x : (int(x.split(',')[2]),x))\n",
    "ordSort = ordPair.sortByKey(ascending=False)\n",
    "for i in ordSort.take(10) : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54657ac1",
   "metadata": {},
   "source": [
    "**What is union?**\n",
    "\n",
    "A union will get all the elements from both the data sets. • In the case of a union, it will not get distinct elements. Apply distinct, if you only want to get distinct elements after union operation.  When we use set operations such as union and intersect, data should have a similar structure (Same Columns and Types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdccb2",
   "metadata": {},
   "source": [
    "**What is Intersection?**\n",
    "\n",
    "Return the intersection of this RDD and another one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec54ff",
   "metadata": {},
   "source": [
    "**Check if duplicates are reported.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d25a6834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd17=sc.parallelize([1,2,3,3,3])\n",
    "rdd18=sc.parallelize([1,3,5])\n",
    "rdd17.intersection(rdd18).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac42de",
   "metadata": {},
   "source": [
    "What is distinct?\n",
    "\n",
    "Return a new RDD containing the distinct elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "30918cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd17.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c94934",
   "metadata": {},
   "source": [
    "**What is subtract**\n",
    "\n",
    "Return each value in left RDD that is not contained in right RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a567a041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd18.subtract(rdd17).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb574de7",
   "metadata": {},
   "source": [
    "**Inspect Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7e59168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "|summary|                id|            weight|            height|               age|gender|\n",
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "|  count|                 7|                 7|                 7|                 7|     7|\n",
      "|   mean|3.2857142857142856| 138.1142857142857| 5.514285714285713|36.142857142857146|  null|\n",
      "| stddev|1.4960264830861913|15.405348483717004|0.3132015933791496|  11.5243014456202|  null|\n",
      "|    min|                 1|             124.1|               5.2|                23|     F|\n",
      "|    max|                 5|             167.2|               5.9|                54|     M|\n",
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[id#501L,weight#502,height#503,age#504L,gender#505]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dtypes #Return df column names and data types\n",
    "df.show() #Display the content of df\n",
    "df.head() #Return first n raws\n",
    "df.first() #Return first row\n",
    "df.take(2) #Return the first n rows \n",
    "df.schema #Return the schema of df\n",
    "df.describe().show() #Compute summary statistics\n",
    "df.columns #Return the columns of df\n",
    "df.count() #Count the number of rows in df\n",
    "df.distinct().count() #Count the number of distinct rows in df\n",
    "df.printSchema() #Print the schema of df\n",
    "df.explain() #Print the (logical and physical) plans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0ed96",
   "metadata": {},
   "source": [
    "**Data Structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7f289424",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd19 = df.rdd #Convert df into an ROD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3f3165d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\":1,\"weight\":144.5,\"height\":5.9,\"age\":33,\"gender\":\"M\"}'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toJSON().first() #Convert df into a ROD of string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d70cd127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.toPandas() #Return the contents of df as Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5d61a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select( \"id\", \"age\").write.save(\"data/idAnage.parquet\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8f1f6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"id\", \"age\").write.save( \"data/idAnage.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2a0fb",
   "metadata": {},
   "source": [
    "**Repartitioning** \n",
    "\n",
    "What is repartition\n",
    "\n",
    "Return a new RDD that has exactly numPartitions partitions. \n",
    "\n",
    " Create almost equal sized partitions. \n",
    "\n",
    "Can increase or decrease the level of parallelism. \n",
    "\n",
    " Spark performs better with equal sized partitions. If you need further processing of huge data, it is preferred to have a equal sized partitions and so we should consider using repartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7af69577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[451] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(4) #New ROD with 4 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ea00d4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoalescedRDD[452] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rdd.coalesce(1) #Decrease the number of partitions in the ROD to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4afdd7",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "50f90e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"data/rdds.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce3e35",
   "metadata": {},
   "source": [
    "**What is Partition ?**\n",
    "\n",
    "Datasets are huge is size and they cannot fit into a single node and so they have to be partitioned across different nodes or machines. \n",
    "\n",
    " Partition in spark is basically an atomic chuck of data stored on a node in the cluster. They are the basic units of parallelism. \n",
    "\n",
    " One partition can not span over multiple machines. \n",
    "\n",
    "Spark automatically partitions RDDs/DataFrames and distributes the partitions across different nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758869a",
   "metadata": {},
   "source": [
    "**What is Coalesce?**\n",
    "\n",
    "Return a new RDD that is reduced into `numPartitions` partitions. \n",
    "\n",
    "Optimized version of repartition(). \n",
    "\n",
    "No shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35e3c7",
   "metadata": {},
   "source": [
    "Reduce number of partitions using coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a1857176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd14 = sc.textFile('data/Orders')\n",
    "rdd14.getNumPartitions()\n",
    "rdd14.coalesce(1).getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c74275",
   "metadata": {},
   "source": [
    "**Does PySpark provide a machine learning API?**\n",
    "\n",
    "Similar to Spark, PySpark provides a machine learning API which is known as MLlib that supports various ML algorithms like:\n",
    "\n",
    "- mllib.classification − This supports different methods for binary or multiclass classification and regression analysis like Random Forest, Decision Tree, Naive Bayes etc.\n",
    "- mllib.clustering − This is used for solving clustering problems that aim in grouping entities subsets with one another depending on similarity.\n",
    "- mllib.fpm − FPM stands for Frequent Pattern Matching. This library is used to mine frequent items, subsequences or other structures that are used for analyzing large datasets.\n",
    "- mllib.linalg − This is used for solving problems on linear algebra.\n",
    "- mllib.recommendation − This is used for collaborative filtering and in recommender systems.\n",
    "- spark.mllib − This is used for supporting model-based collaborative filtering where small latent factors are identified using the Alternating Least Squares (ALS) algorithm which is used for predicting missing entries.\n",
    "- mllib.regression − This is used for solving problems using regression algorithms that find relationships and variable dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11fe303",
   "metadata": {},
   "source": [
    "**Is PySpark faster than pandas?**\n",
    "\n",
    "PySpark supports parallel execution of statements in a distributed environment, i.e on different cores and different machines which are not present in Pandas. This is why PySpark is faster than pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a247b",
   "metadata": {},
   "source": [
    "**What is Broadcast Variables?**\n",
    "\n",
    "Broadcast variables: These are also known as read-only shared variables and are used in cases of data lookup requirements. These variables are cached and are made available on all the cluster nodes so that the tasks can make use of them. The variables are not sent with every task. They are rather distributed to the nodes using efficient algorithms for reducing the cost of communication. When we run an RDD job operation that makes use of Broadcast variables, the following things are done by PySpark:\n",
    "\n",
    "The job is broken into different stages having distributed shuffling. The actions are executed in those stages.\n",
    "The stages are then broken into tasks.\n",
    "The broadcast variables are broadcasted to the tasks if the tasks need to use it.\n",
    "Broadcast variables are created in PySpark by making use of the broadcast(variable) method from the SparkContext class. The syntax for this goes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "42e91637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 22, 31]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = sc.broadcast([10, 11, 22, 31])\n",
    "broadcastVar.value    # access broadcast variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6fcc5",
   "metadata": {},
   "source": [
    "An important point of using broadcast variables is that the variables are not sent to the tasks when the broadcast function is called. They will be sent when the variables are first required by the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40294d3a",
   "metadata": {},
   "source": [
    "**When to use Broadcast Variable?**\n",
    "\n",
    "For processing, the executors need information regarding variables or methods. This information is serialized by Spark and sent to each executor and is known as CLOSURE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd5968",
   "metadata": {},
   "source": [
    "**What is Accumulator variable?**\n",
    "\n",
    "Accumulator variables: These variables are called updatable shared variables. They are added through associative and commutative operations and are used for performing counter or sum operations. PySpark supports the creation of numeric type accumulators by default. It also has the ability to add custom accumulator types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e952e55",
   "metadata": {},
   "source": [
    "**What is PySpark Architecture?**\n",
    "\n",
    "PySpark similar to Apache Spark works in master-slave architecture pattern. Here, the master node is called the Driver and the slave nodes are called the workers. When a Spark application is run, the Spark Driver creates SparkContext which acts as an entry point to the spark application. All the operations are executed on the worker nodes. The resources required for executing the operations on the worker nodes are managed by the Cluster Managers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041dd4c",
   "metadata": {},
   "source": [
    "**What is the common workflow of a spark program?**\n",
    "\n",
    "The most common workflow followed by the spark program is:\n",
    "The first step is to create input RDDs depending on the external data.\n",
    "Data can be obtained from different data sources.\n",
    "Post RDD creation, the RDD transformation operations like filter() or map() are run for creating new RDDs depending on the business logic.\n",
    "If any intermediate RDDs are required to be reused for later purposes, we can persist those RDDs.\n",
    "Lastly, if any action operations like first(), count() etc are present then spark launches it to initiate parallel computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353eeffd",
   "metadata": {},
   "source": [
    "**Congratulation!** We have practiced Pyspark with Jupyter Notebook in Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402abbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Pyspark)",
   "language": "python",
   "name": "pyspark38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
